{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import time\r\n",
    "import math\r\n",
    "import tables\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torchvision.models as models\r\n",
    "from torchvision import transforms\r\n",
    "from torch.utils.tensorboard import SummaryWriter\r\n",
    "\r\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Dataset(object):\r\n",
    "\t'''\r\n",
    "\tAssumptions made about the dataset:\r\n",
    "\t\r\n",
    "\tPytable contains associated hdf5 earrays'''\r\n",
    "\tdef __init__(self, fname, img_transform):\r\n",
    "\t\t#nothing special here, just internalizing the constructor parameters\r\n",
    "\t\tself.fname=fname\r\n",
    "\r\n",
    "\t\tself.img_transform=img_transform\r\n",
    "\t\t\t\r\n",
    "\t\twith tables.open_file(self.fname,'r') as db:\r\n",
    "\t\t\tself.classsizes=db.root.classsizes[:]\r\n",
    "\t\t\tself.nitems=db.root.imgs.shape[0]\r\n",
    "\t\t\t\r\n",
    "\t\tself.imgs = None\r\n",
    "\t\tself.labels = None\r\n",
    "\t\tself.slide_ids = None\r\n",
    "\t\tself.mode = 1\r\n",
    "\t\tself.idxs = None\r\n",
    "\r\n",
    "\tdef get_slide_ids(self):\r\n",
    "\t\twith tables.open_file(self.fname,'r') as db:\r\n",
    "\t\t\treturn torch.tensor(np.array(db.root.slide_ids))\r\n",
    "\t\t\r\n",
    "\tdef get_labels(self):\r\n",
    "\t\twith tables.open_file(self.fname,'r') as db:\r\n",
    "\t\t\treturn torch.tensor(np.array(db.root.labels))\r\n",
    "\r\n",
    "\tdef setmode(self, mode):\r\n",
    "\t\tself.mode = mode\r\n",
    "\r\n",
    "\tdef make_train_data(self, idxs):\r\n",
    "\t\t\"\"\"creates a training set based on the topk highest probability patches\r\n",
    "\t\t\r\n",
    "\t\t\tArgs:\r\n",
    "\t\t\t\tidxs (int[]): array of patch ids. \r\n",
    "\t\t\r\n",
    "\t\t\tReturns: \r\n",
    "\t\t\t\tNone: \r\n",
    "\t\t\"\"\"\r\n",
    "\t\tself.idxs = idxs\r\n",
    "\r\n",
    "\tdef shuffle_training_data(self):\r\n",
    "\t\tself.idxs = random.sample(self.idxs, len(self.idxs))\r\n",
    "\t\t\t\r\n",
    "\tdef __getitem__(self, index):\r\n",
    "\t\tif self.mode==1:\t# without topk grouping: the full dataset\r\n",
    "\t\t\t#opening should be done in __init__ but seems to be\r\n",
    "\t\t\t#an issue with multithreading so doing here. need to do it everytime, otherwise hdf5 crashes\r\n",
    "\t\t\twith tables.open_file(self.fname,'r') as db:\r\n",
    "\t\t\t\tself.imgs=db.root.imgs\r\n",
    "\t\t\t\tself.labels=db.root.labels\r\n",
    "\t\t\t\tself.slide_ids=db.root.slide_ids\r\n",
    "\r\n",
    "\t\t\t\t#get the requested image\r\n",
    "\t\t\t\timg = self.imgs[index,::]\r\n",
    "\t\t\t\tlabel = self.labels[index]\r\n",
    "\t\t\t\tslide_id = self.slide_ids[index]\r\n",
    "\t\t\t\r\n",
    "\t\t\tif self.img_transform is not None:\r\n",
    "\t\t\t\timg_new = self.img_transform(img)\r\n",
    "\r\n",
    "\t\t\treturn img_new, label, img, slide_id\r\n",
    "\r\n",
    "\t\telif self.mode==2:\t# topk sampling: only the top k instances from each slide, compiled.\r\n",
    "\t\t\twith tables.open_file(self.fname,'r') as db:\r\n",
    "\t\t\t\tself.imgs=db.root.imgs\r\n",
    "\t\t\t\tself.labels=db.root.labels\r\n",
    "\t\t\t\tself.slide_ids=db.root.slide_ids\r\n",
    "\r\n",
    "\t\t\t\thigh_prob_tile_index = self.idxs[index]\t# converts input index to the index within interesting patches.\r\n",
    "\r\n",
    "\t\t\t\timg = self.imgs[high_prob_tile_index,::]\r\n",
    "\t\t\t\tlabel = self.labels[high_prob_tile_index]\r\n",
    "\t\t\t\tslide_id = self.slide_ids[high_prob_tile_index]\r\n",
    "\r\n",
    "\t\t\tif self.img_transform is not None:\r\n",
    "\t\t\t\timg_new = self.img_transform(img)\r\n",
    "\r\n",
    "\t\t\treturn img_new, label, img, slide_id\r\n",
    "\t\telse:\r\n",
    "\t\t\traise ValueError('Mode has not been set to either 1 or 2')\r\n",
    "\r\n",
    "\tdef __len__(self):\r\n",
    "\t\tif self.mode == 1:\r\n",
    "\t\t\treturn self.nitems\r\n",
    "\t\telif self.mode == 2:\r\n",
    "\t\t\treturn len(self.idxs)\r\n",
    "\t\telse:\r\n",
    "\t\t\treturn None\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#helper function for pretty printing of current time and remaining time\r\n",
    "def asMinutes(s):\r\n",
    "    m = math.floor(s / 60)\r\n",
    "    s -= m * 60\r\n",
    "    return '%dm %ds' % (m, s)\r\n",
    "def timeSince(since, percent):\r\n",
    "    now = time.time()\r\n",
    "    s = now - since\r\n",
    "    es = s / (percent+.00001)\r\n",
    "    rs = es - s\r\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def prepare_dataset(dataname, batch_size, phases):\r\n",
    "\t\"\"\"Prepares a dataset based on a pre-created pytable set.\r\n",
    "\t\r\n",
    "\t\tArgs:\r\n",
    "\t\t\tdataname (str): name of the data\r\n",
    "\t\t\tphase (str): target phase\r\n",
    "\t\t\tbatch_size: size of each batch\r\n",
    "\t\t\r\n",
    "\t\tReturns: \r\n",
    "\t\t\tdataset: dataset object\r\n",
    "\t\t\tdataloader: used to load data directly into the model\r\n",
    "\t\"\"\"\r\n",
    "\t\r\n",
    "\timg_transform = transforms.Compose([\r\n",
    "    transforms.ToPILImage(),\r\n",
    "    # transforms.RandomVerticalFlip(),\r\n",
    "    # transforms.RandomHorizontalFlip(),\r\n",
    "    # transforms.RandomCrop(size=(patch_size,patch_size),pad_if_needed=True), #these need to be in a reproducible order, first affine transforms and then color\r\n",
    "    transforms.ToTensor()\r\n",
    "    ])\r\n",
    "\r\n",
    "\tdataset={}\r\n",
    "\tdataLoader={}\r\n",
    "\tfor phase in phases: #now for each of the phases, we're creating the dataloader\r\n",
    "\t\t\t\t\t\t#interestingly, given the batch size, i've not seen any improvements from using a num_workers>0\r\n",
    "\t\t\r\n",
    "\t\tdataset[phase]=Dataset(f\"./data/{dataname}_{phase}.pytable\", img_transform=img_transform)\r\n",
    "\t\tdataLoader[phase]=DataLoader(dataset[phase], batch_size=batch_size, \r\n",
    "\t\t\t\t\t\t\t\t\tshuffle=False, num_workers=0,pin_memory=True, drop_last=False) \r\n",
    "\t\tprint(f\"{phase} dataset size:\\t{len(dataset[phase])}\")\r\n",
    "\r\n",
    "\treturn dataset, dataLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def define_model(dataname, dataset, device, load_weights=False):\r\n",
    "\t\"\"\"creates the model object and associated tools\r\n",
    "\t\r\n",
    "\t\tArgs:\r\n",
    "\t\t\tdataname (str): name of the data\r\n",
    "\t\t\tdataset (Dataset): dataset object\r\n",
    "\t\t\tdevice (): processing unit to use\r\n",
    "\t\t\tload_weights (bool): whether or not to load pretrained\r\n",
    "\t\t\t\t\t\t\t\tweights from a .pth file\r\n",
    "\t\r\n",
    "\t\tReturns: \r\n",
    "\t\t\tmodel: resnet18 model\r\n",
    "\t\t\tcriterion: a CrossEntropyLoss object\r\n",
    "\t\t\toptimizer: an Adam optimizer\r\n",
    "\t\"\"\"\r\n",
    "\t\r\n",
    "\tmodel = models.resnet18()\r\n",
    "\tmodel.fc = nn.Linear(model.fc.in_features, 2)\r\n",
    "\tmodel.to(device)\r\n",
    "\r\n",
    "\tclass_weight=dataset[\"train\"].classsizes\r\n",
    "\tclass_weight=torch.from_numpy(1-class_weight/class_weight.sum()).type('torch.FloatTensor').to(device)\r\n",
    "\tcriterion = nn.CrossEntropyLoss(weight=class_weight)\r\n",
    "\toptimizer = torch.optim.Adam(model.parameters())\r\n",
    "\r\n",
    "\tif load_weights:\r\n",
    "\t\tcheckpoint = torch.load(f\"{dataname}_unet_best_model.pth\")\r\n",
    "\t\tmodel.load_state_dict(checkpoint['model_dict'])\r\n",
    "\r\n",
    "\treturn model, criterion, optimizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def define_device(gpuid):\r\n",
    "\t\"\"\"specify if we should use a GPU (cuda) or only the CPU\r\n",
    "\t\r\n",
    "\t\tArgs:\r\n",
    "\t\t\tgpuid (int): which gpu is being used \r\n",
    "\t\r\n",
    "\t\tReturns: \r\n",
    "\t\t\tdevice: the gpu (cuda) or the cpu\r\n",
    "\t\"\"\"\r\n",
    "\t\r\n",
    "\tif(torch.cuda.is_available()):\r\n",
    "\t\tprint(torch.cuda.get_device_properties(gpuid))\r\n",
    "\t\ttorch.cuda.set_device(gpuid)\r\n",
    "\t\tdevice = torch.device(f'cuda:{gpuid}')\r\n",
    "\telse:\r\n",
    "\t\tdevice = torch.device(f'cpu')\r\n",
    "\r\n",
    "\treturn device"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def infer(run, dataLoader, dataset, model, phase, device, batchsize):\r\n",
    "\t\r\n",
    "\tmodel.eval()\r\n",
    "\tprobs = torch.FloatTensor(len(dataset[phase]))\r\n",
    "\tfor ii, (X, label, img_orig, slide_id) in enumerate(dataLoader[phase]):\r\n",
    "\t\tX = X.to(device)  # [Nbatch, 3, H, W]\r\n",
    "\t\tlabel = label.type('torch.LongTensor').to(device)\r\n",
    "\t\toutput = F.softmax(model(X), dim=1)\r\n",
    "\t\tprobs[ii*batchsize : ii*batchsize+X.shape[0]] = output.detach()[:,1].clone()\r\n",
    "\t\r\n",
    "\treturn probs.cpu().numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def train(run, dataLoader, model, criterion, optimizer, device):\r\n",
    "\tmodel.train()\r\n",
    "\trunning_loss = 0.\r\n",
    "\tfor ii, (X, label, img_orig, slide_id) in enumerate(dataLoader['train']): # where ii is the batch number\r\n",
    "\t\tprint(f\"batch={ii} epoch={run}\")\r\n",
    "\t\tprint(f\"X.shape={X.shape}\")\r\n",
    "\t\tX = X.to(device)\r\n",
    "\t\tlabel = label.type('torch.LongTensor').to(device)\r\n",
    "\r\n",
    "\t\toutput = model(X)\r\n",
    "\t\tloss = criterion(output, label)\r\n",
    "\t\toptimizer.zero_grad()\r\n",
    "\t\tloss.backward()\r\n",
    "\t\toptimizer.step()\r\n",
    "\t\r\n",
    "\treturn loss\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def calc_err(pred,real):\r\n",
    "\t\"\"\"error analysis during validation\r\n",
    "\t\r\n",
    "\t\tArgs:\r\n",
    "\t\t\tpred (int[]): the array of 0/1 predictions of the model\r\n",
    "\t\t\treal (int[]): the array of ground-truth labels \r\n",
    "\t\r\n",
    "\t\tReturns: \r\n",
    "\t\t\terr (float): the err generated from pred and real\r\n",
    "\t\t\tfpr (float): false positive rate\r\n",
    "\t\t\tfnr (float): false negative rate\r\n",
    "\t\"\"\"\r\n",
    "\r\n",
    "\tpred = np.array(pred)\r\n",
    "\treal = np.array(real)\r\n",
    "\tneq = np.not_equal(pred, real)\r\n",
    "\terr = float(neq.sum())/pred.shape[0]\r\n",
    "\tfpr = float(np.logical_and(pred==1,neq).sum())/(real==0).sum()\r\n",
    "\tfnr = float(np.logical_and(pred==0,neq).sum())/(real==1).sum()\r\n",
    "\treturn err, fpr, fnr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def group_argtopk(groups, data,k=1):\r\n",
    "\t\"\"\"Outputs the index of the instance with the highest probability for each spot. \r\n",
    "\t\r\n",
    "\t\tArgs:\r\n",
    "\t\t\tgroups (int[]): list of slide ids for each instance, ex. [1,1,5,4,7,4,7,7, ...]\r\n",
    "\t\t\tdata (float[]): list of probabilities for each instance, ex. [.7,.8,.1,.4,.5, ...]\r\n",
    "\t\tReturns: \r\n",
    "\t\t\tint[]: k indices for each slide, which include the highest inferred probabilities for that slide.\r\n",
    "\t\t\t\t   ex. index 0 refers to the highest prob for the 0th slide,\r\n",
    "\t\t\t\t\t   index 1 refers to the highest prob for the 1st slide,\r\n",
    "\t\t\t\t\t   ...\r\n",
    "\t\"\"\"\r\n",
    "\t\r\n",
    "\torder = np.lexsort((data, groups))\r\n",
    "\tgroups = groups[order]\r\n",
    "\tdata = data[order]\r\n",
    "\tindex = np.empty(len(groups), 'bool')\r\n",
    "\tindex[-k:] = True\r\n",
    "\tindex[:-k] = groups[k:] != groups[:-k]\r\n",
    "\treturn list(order[index])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def group_max(groups, data, nmax):\r\n",
    "\t\"\"\"description\r\n",
    "\r\n",
    "\t\tArgs:\r\n",
    "\t\t\tparamname (paramtype): describe_the_param \r\n",
    "\r\n",
    "\t\tReturns: \r\n",
    "\t\t\ttype: description\r\n",
    "\t\"\"\"\r\n",
    "\tout = np.empty(nmax)\r\n",
    "\tout[:] = np.nan\r\n",
    "\torder = np.lexsort((data, groups))\r\n",
    "\tgroups = groups[order]\r\n",
    "\tdata = data[order]\r\n",
    "\tindex = np.empty(len(groups), 'bool')\r\n",
    "\tindex[-1] = True\r\n",
    "\tindex[:-1] = groups[1:] != groups[:-1]\r\n",
    "\tout[groups[index]] = data[index]\r\n",
    "\treturn out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def save_to_tensorboard(writer, phase, all_loss, all_acc, epoch, cmatrix):\r\n",
    "\tall_acc[phase]=(cmatrix[phase]/cmatrix[phase].sum()).trace()\r\n",
    "\tall_loss[phase] = all_loss[phase].cpu().numpy().mean()\r\n",
    "\r\n",
    "\twriter.add_scalar(f'{phase}/loss', all_loss[phase], epoch)\r\n",
    "\r\n",
    "\tif phase == 'val':\r\n",
    "\t\twriter.add_scalar(f'{phase}/acc', all_acc[phase], epoch)\r\n",
    "\t\tfor r in range(2):\r\n",
    "\t\t\tfor c in range(2): #essentially write out confusion matrix\r\n",
    "\t\t\t\twriter.add_scalar(f'{phase}/{r}{c}', cmatrix[phase][r][c],epoch)\r\n",
    "\t"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def generate_single_output(dataname, phase, imgid, model):\r\n",
    "\t'''\r\n",
    "\tWORK IN PROGRESS'''\r\n",
    "\tdb = tables.open_file(f\"./{dataname}_{phase}.pytable\")\r\n",
    "\timg = db.root.imgs[imgid, ::]\r\n",
    "\tlabel = torch.tensor(np.array(db.root.labels[imgid]))\r\n",
    "\tfig, ax = plt.subplots(1, 2)\r\n",
    "\tax = ax.flatten()\r\n",
    "\r\n",
    "\toutput = model(img)\r\n",
    "\r\n",
    "\tax[0].set_title()\r\n",
    "\tax[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# --- general parameters\r\n",
    "dataname = 'MIL_mini_32'\r\n",
    "gpuid=0\r\n",
    "k=50\r\n",
    "\r\n",
    "# --- resnet params\r\n",
    "n_classes = 2\r\n",
    "in_channels = 3\r\n",
    "\r\n",
    "# --- evaluation params\r\n",
    "batch_size=64\r\n",
    "patch_size=32 #based on resnet architecture. Changed from 224\r\n",
    "num_epochs=10\r\n",
    "phases = [\"train\",\"val\"] #how many phases did we create databases for?\r\n",
    "\r\n",
    "# --- initializing model and dataset\r\n",
    "writer = SummaryWriter()\r\n",
    "device = define_device(gpuid)\r\n",
    "dataset, dataLoader = prepare_dataset(dataname, batch_size, phases)\r\n",
    "model, criterion, optimizer = define_model(dataname, dataset, device)\r\n",
    "best_loss_on_test = np.Infinity\r\n",
    "\r\n",
    "for epoch in range(num_epochs):\r\n",
    "\tall_acc = {key: 0 for key in phases} \r\n",
    "\tall_loss = {key: torch.zeros(0).to(device) for key in phases} #keep this on GPU for greatly improved performance\r\n",
    "\tcmatrix = {key: np.zeros((n_classes,n_classes)) for key in phases}\r\n",
    "\t\r\n",
    "\tfor phase in phases:\r\n",
    "\t\t'''\r\n",
    "\t\tIf phase is train, model will:\r\n",
    "\t\t1. infer probabilities for each instance in the training set\r\n",
    "\t\t2. group the highest probability instances for each WSI\r\n",
    "\t\t3. train the model based on these instances\r\n",
    "\t\t4. log the loss.\r\n",
    "\t\t'''\r\n",
    "\t\tlabels = dataset[phase].get_labels()\r\n",
    "\t\tslide_ids = dataset[phase].get_slide_ids()\r\n",
    "\r\n",
    "\t\tif phase == 'train':\r\n",
    "\t\t\tdataset[phase].setmode(1)\r\n",
    "\t\t\tprobs = infer(epoch, dataLoader, dataset, model, phase, device, batch_size)\r\n",
    "\t\t\ttopk = group_argtopk(slide_ids, probs, k)\r\n",
    "\t\t\tdataset[phase].make_train_data(topk)\t\t\t\t\t\t\t\t# this will need to be tested to see if it works\r\n",
    "\t\t\tdataset[phase].shuffle_training_data()\r\n",
    "\r\n",
    "\t\t\tdataset[phase].setmode(2)\r\n",
    "\t\t\tloss = train(epoch, dataLoader, model, criterion, optimizer, device)\r\n",
    "\t\t\tall_loss[phase]=torch.cat((all_loss[phase],loss.detach().view(1,-1)))\r\n",
    "\t\t\tprint('Training\\tEpoch: [{}/{}]\\tLoss: {}'.format(epoch+1, num_epochs, loss))\r\n",
    "\r\n",
    "\t\t'''\r\n",
    "\t\tIf phase is val, model will:\r\n",
    "\t\t1. infer probabilities\r\n",
    "\t\t2. identify the maximum probabilities'''\r\n",
    "\t\tif phase == 'val':\r\n",
    "\t\t\t# do error analysis\r\n",
    "\t\t\tdataset[phase].setmode(1)\r\n",
    "\t\t\tprobs = infer(epoch, dataLoader, dataset, model, phase, device, batch_size)\r\n",
    "\t\t\tmaxs = group_max(slide_ids, probs, len(slide_ids))\r\n",
    "\t\t\tpred = [1 if x >= 0.5 else 0 for x in maxs]\r\n",
    "\t\t\terr,fpr,fnr = calc_err(pred, labels)\r\n",
    "\t\t\t#cmatrix[phase]=cmatrix[phase]+confusion_matrix(yflat,cpredflat, labels=range(nclasses))\r\n",
    "\t\t\tprint(f'error: {err}, false positive rate: {fpr}, false negative rate: {fnr}')\r\n",
    "\r\n",
    "\t\tsave_to_tensorboard(writer, phase, all_loss, all_acc, epoch, cmatrix)\r\n",
    "\t\r\n",
    "\t# save state of model if it's the best one so far.\r\n",
    "\tif all_loss[\"val\"] < best_loss_on_test:\r\n",
    "\t\tbest_loss_on_test = all_loss[\"val\"]\r\n",
    "\t\tprint(\"  **\")\r\n",
    "\t\tstate = {'epoch': epoch + 1,\r\n",
    "\t\t'model_dict': model.state_dict(),\r\n",
    "\t\t'optim_dict': optimizer.state_dict(),\r\n",
    "\t\t'best_loss_on_test': all_loss,\r\n",
    "\t\t'n_classes': n_classes}\r\n",
    "\t\ttorch.save(state, f\"{dataname}_resnet_best_model.pth\")\r\n",
    "\telse:\r\n",
    "\t\tprint(\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train dataset size:\t5000\n",
      "val dataset size:\t1000\n",
      "batch=0 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=1 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=2 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=3 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=4 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=5 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=6 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=7 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=8 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=9 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=10 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=11 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=12 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=13 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=14 epoch=0\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=15 epoch=0\n",
      "X.shape=torch.Size([40, 3, 32, 32])\n",
      "Training\tEpoch: [1/10]\tLoss: 0.43130481243133545\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-12-e295964a63a5>:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  all_acc[phase]=(cmatrix[phase]/cmatrix[phase].sum()).trace()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "error: 0.497, false positive rate: 0.0, false negative rate: 1.0\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-12-e295964a63a5>:3: RuntimeWarning: Mean of empty slice.\n",
      "  all_loss[phase] = all_loss[phase].cpu().numpy().mean()\n",
      "C:\\Users\\dsjny\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "batch=0 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=1 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=2 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=3 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=4 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=5 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=6 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=7 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=8 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=9 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=10 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=11 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=12 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=13 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=14 epoch=1\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=15 epoch=1\n",
      "X.shape=torch.Size([40, 3, 32, 32])\n",
      "Training\tEpoch: [2/10]\tLoss: 0.4484308063983917\n",
      "error: 0.497, false positive rate: 0.0, false negative rate: 1.0\n",
      "\n",
      "batch=0 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=1 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=2 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=3 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=4 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=5 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=6 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=7 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=8 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=9 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=10 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=11 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=12 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=13 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=14 epoch=2\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=15 epoch=2\n",
      "X.shape=torch.Size([40, 3, 32, 32])\n",
      "Training\tEpoch: [3/10]\tLoss: 0.4996120035648346\n",
      "error: 0.497, false positive rate: 0.0, false negative rate: 1.0\n",
      "\n",
      "batch=0 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=1 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=2 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=3 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=4 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=5 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=6 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=7 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=8 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=9 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=10 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=11 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=12 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=13 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=14 epoch=3\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=15 epoch=3\n",
      "X.shape=torch.Size([40, 3, 32, 32])\n",
      "Training\tEpoch: [4/10]\tLoss: 0.14282186329364777\n",
      "error: 0.497, false positive rate: 0.0019880715705765406, false negative rate: 0.9979879275653923\n",
      "\n",
      "batch=0 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=1 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=2 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=3 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=4 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=5 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=6 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=7 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=8 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=9 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=10 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=11 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=12 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=13 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=14 epoch=4\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=15 epoch=4\n",
      "X.shape=torch.Size([40, 3, 32, 32])\n",
      "Training\tEpoch: [5/10]\tLoss: 0.15097472071647644\n",
      "error: 0.495, false positive rate: 0.0019880715705765406, false negative rate: 0.993963782696177\n",
      "\n",
      "batch=0 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=1 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=2 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=3 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=4 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=5 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=6 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=7 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=8 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=9 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=10 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=11 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=12 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=13 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=14 epoch=5\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=15 epoch=5\n",
      "X.shape=torch.Size([40, 3, 32, 32])\n",
      "Training\tEpoch: [6/10]\tLoss: 0.21737587451934814\n",
      "error: 0.495, false positive rate: 0.0019880715705765406, false negative rate: 0.993963782696177\n",
      "\n",
      "batch=0 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=1 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=2 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=3 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=4 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=5 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=6 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=7 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=8 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=9 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=10 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=11 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=12 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=13 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=14 epoch=6\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=15 epoch=6\n",
      "X.shape=torch.Size([40, 3, 32, 32])\n",
      "Training\tEpoch: [7/10]\tLoss: 0.027604831382632256\n",
      "error: 0.495, false positive rate: 0.0019880715705765406, false negative rate: 0.993963782696177\n",
      "\n",
      "batch=0 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=1 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=2 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=3 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=4 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=5 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=6 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=7 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=8 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=9 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=10 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=11 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=12 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=13 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=14 epoch=7\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=15 epoch=7\n",
      "X.shape=torch.Size([40, 3, 32, 32])\n",
      "Training\tEpoch: [8/10]\tLoss: 0.03214745968580246\n",
      "error: 0.495, false positive rate: 0.0019880715705765406, false negative rate: 0.993963782696177\n",
      "\n",
      "batch=0 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=1 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=2 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=3 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=4 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=5 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=6 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=7 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=8 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=9 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=10 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=11 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=12 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=13 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=14 epoch=8\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=15 epoch=8\n",
      "X.shape=torch.Size([40, 3, 32, 32])\n",
      "Training\tEpoch: [9/10]\tLoss: 0.04770195111632347\n",
      "error: 0.495, false positive rate: 0.0019880715705765406, false negative rate: 0.993963782696177\n",
      "\n",
      "batch=0 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=1 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=2 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=3 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=4 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=5 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=6 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=7 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=8 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=9 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=10 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=11 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=12 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=13 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=14 epoch=9\n",
      "X.shape=torch.Size([64, 3, 32, 32])\n",
      "batch=15 epoch=9\n",
      "X.shape=torch.Size([40, 3, 32, 32])\n",
      "Training\tEpoch: [10/10]\tLoss: 0.04181739687919617\n",
      "error: 0.495, false positive rate: 0.0019880715705765406, false negative rate: 0.993963782696177\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#for ii, (X, label, img_orig, slide_id) in enumerate(dataLoader['train']):\r\n",
    "#\tprint(ii)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "e06a423795a8f32368fd1762cb05e83bf5233f8c2906c097409655eb60b084c3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}